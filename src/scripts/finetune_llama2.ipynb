{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fine-tune Llama 2 for Sentiment Analysis\n",
    "\n",
    "The FinancialPhraseBank dataset is a comprehensive collection that captures the sentiments of financial news headlines from the viewpoint of a retail investor. Comprising two key columns, namely \"Sentiment\" and \"News Headline,\" the dataset effectively classifies sentiments as either negative, neutral, or positive. This structured dataset serves as a valuable resource for analyzing and understanding the complex dynamics of sentiment in the domain of financial news. It has been used in various studies and research initiatives, since its inception in the work by Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P.  \"Good debt or bad debt: Detecting semantic orientations in economic texts.\", published in the Journal of the Association for Information Science and Technology in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* accelerate is a distributed training library for PyTorch by HuggingFace. It allows you to train your models on multiple GPUs or CPUs in parallel (distributed configurations), which can significantly speed up training in presence of multiple GPUs (we won't use it in our example).\n",
    "* peft is a Python library by HuggingFace for efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n",
    "* bitsandbytes by Tim Dettmers, is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32, and so on).\n",
    "* transformers is a Python library for natural language processing (NLP). It provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n",
    "* trl is a full stack library by HuggingFace providing a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U \"torch==2.1.2\" tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "petals 2.3.0.dev2 requires accelerate>=0.27.2, but you have accelerate 0.26.1 which is incompatible.\n",
      "petals 2.3.0.dev2 requires bitsandbytes==0.41.1, but you have bitsandbytes 0.42.0 which is incompatible.\n",
      "petals 2.3.0.dev2 requires peft==0.5.0, but you have peft 0.7.2.dev0 which is incompatible.\n",
      "petals 2.3.0.dev2 requires transformers==4.37.1, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
    "!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Initialization\n",
    "Import libraries and testing the available GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell there are all the other imports for running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 14.485002,
     "end_time": "2023-10-16T11:00:18.917449",
     "exception": false,
     "start_time": "2023-10-16T11:00:04.432447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, PeftConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version 2.1.2+cu121\n",
      "working on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libs \n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data and the core evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell performs the following steps:\n",
    "\n",
    "1. Reads the input dataset from the all-data.csv file, which is a comma-separated value (CSV) file with two columns: sentiment and text.\n",
    "2. Splits the dataset into training and test sets, with 300 samples in each set. The split is stratified by sentiment, so that each set contains a representative sample of positive, neutral, and negative sentiments.\n",
    "3. Shuffles the train data in a replicable order (random_state=10)\n",
    "4. Clean text (remove url, stopwords, non-word, shortword)\n",
    "5. Transforms the texts contained in the train and test data into prompts to be used by Llama: the train prompts contains the expected answer we want to fine-tune the model with\n",
    "\n",
    "This prepares in a single cell train_data, eval_data and test_data datasets to be used in our fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iav1hc/miniconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/iav1hc/miniconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from libs.data_processor import DATA_PROCESSOR\n",
    "\n",
    "filename = \"../../data/all-data.csv\"\n",
    "MAXLENGTH = 128\n",
    "data_processor = DATA_PROCESSOR(maxlen = MAXLENGTH, prompt_mode = True)\n",
    "data_processor.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Mr Jortikka is president of the base metal div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>positive</td>\n",
       "      <td>Both operating profit and net sales for the 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>negative</td>\n",
       "      <td>Finnish automation solutions developer Cencorp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>positive</td>\n",
       "      <td>Renzo Piano 's building design will be a wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>positive</td>\n",
       "      <td>`` We are proud to contribute to the creation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>neutral</td>\n",
       "      <td>The dividend will be paid on April 15 , 2008 t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>neutral</td>\n",
       "      <td>The new shares entitle their holders to divide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Activities range from the development of natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to Bosse , the present cooperation i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit rose to 22.1 mln eur from 19....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text\n",
       "3683   neutral  Mr Jortikka is president of the base metal div...\n",
       "163   positive  Both operating profit and net sales for the 12...\n",
       "4017  negative  Finnish automation solutions developer Cencorp...\n",
       "1588  positive  Renzo Piano 's building design will be a wonde...\n",
       "1799  positive  `` We are proud to contribute to the creation ...\n",
       "...        ...                                                ...\n",
       "1374   neutral  The dividend will be paid on April 15 , 2008 t...\n",
       "3869   neutral  The new shares entitle their holders to divide...\n",
       "2766   neutral  Activities range from the development of natur...\n",
       "1798  positive  According to Bosse , the present cooperation i...\n",
       "243   positive  Operating profit rose to 22.1 mln eur from 19....\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processor.X_train_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use LLAMA2Classifier for preparing model for finetuning phase. The model \"NousResearch/Llama-2-7b-chat-hf\" is selected since the original Llama2 model requires a Huggingface account and then authorization. It makes our demo more complicated\n",
    "Before finetuning, we evaluate the performance of the pre-training model \"NousResearch/Llama-2-7b-chat-hf\" on the test dataset as follows:\n",
    "1. Maps the sentiment labels to a numerical representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n",
    "2. Calculates the accuracy of the model on the test data.\n",
    "3. Generates an accuracy report for each sentiment label.\n",
    "4. Generates a classification report for the model.\n",
    "5. Generates a confusion matrix for the model.\n",
    "\n",
    "The model \"NousResearch/Llama-2-7b-chat-hf\" is hard-coded in the class, but can be reselected with any LLM-GPT. However, they have not tested yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iav1hc/miniconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.14s/it]\n",
      " 15%|████████████                                                                   | 137/900 [16:22<1:32:29,  7.27s/it]"
     ]
    }
   ],
   "source": [
    "from libs.classifiers.llama2 import LLAMA2Classifier\n",
    "predictor = LLAMA2Classifier()\n",
    "\n",
    "predictor.build_model()\n",
    "predictor.evaluate(data_processor, name = \"llama2_org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Finetuning model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFTConfig:\n",
    "\n",
    "The peft_config object specifies the parameters for PEFT. The following are some of the most important parameters:\n",
    "\n",
    "* lora_alpha: The learning rate for the LoRA update matrices.\n",
    "* lora_dropout: The dropout probability for the LoRA update matrices.\n",
    "* r: The rank of the LoRA update matrices.\n",
    "* bias: The type of bias to use. The possible values are none, additive, and learned.\n",
    "* task_type: The type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM.\n",
    "  \n",
    "Understanding LoRA parameters to avoid overfitting issues.\n",
    "* A lower LoRA rank (e.g., 8 or 16) is suitable for fine-tuning on a base model, where the goal is to adapt the model to a specific task or dataset. In this case, the model is not required to learn new concepts, but rather to adjust its existing knowledge to fit the new task.\n",
    "A higher LoRA rank (e.g., 32 or 64) is more suitable for teaching the model new concepts or adapting it to a significantly different dataset. This is because a higher rank allows the model to learn more complex patterns and relationships in the data.\n",
    "* Alpha is a scaling parameter that controls the strength of the low-rank approximation. A higher alpha value places more emphasis on the low-rank structure, while a lower value reduces its influence. When adjusting r and alpha, consider their interplay. Increasing r without adjusting alpha can lead to overfitting, while increasing alpha without adjusting r can result in underfitting. A balanced approach involves incrementing r and alpha simultaneously to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 9570.38 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 12068.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iav1hc/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='287' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [287/336 48:09 < 08:16, 0.10 it/s, Epoch 2.54/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>0.862645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.812189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "                lora_alpha=16, \n",
    "                lora_dropout=0.1,\n",
    "                r=32,\n",
    "                bias=\"none\",\n",
    "                #target_modules=\"all-linear\",\n",
    "                target_modules = [\n",
    "                        \"q_proj\",\n",
    "                        #\"up_proj\",\n",
    "                        \"o_proj\",\n",
    "                        \"k_proj\",\n",
    "                        #\"down_proj\",\n",
    "                        #\"gate_proj\",\n",
    "                        \"v_proj\"\n",
    "                        ],\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "predictor.train(data_processor=data_processor, peft_config=peft_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5_finetuned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'show'"
     ]
    }
   ],
   "source": [
    "predictor.evaluate(data_processor, name = \"t5_finetuned\", show= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 7b-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on a text news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging weight with pre-trained model\n",
    "Then we can proceed to merging the weights and we will be using the merged model for our testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iav1hc/miniconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \"./trained_weigths/\"\n",
    "from libs.classifiers.llama2 import LLAMA2Classifier\n",
    "classifer = LLAMA2Classifier()\n",
    "classifer.merge_finetuned_model(finetuned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████▊                    | 664/900 [2:58:43<1:04:29, 16.40s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_test = data_processor.X_test_org\n",
    "y_pred = []\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    prompt = X_test.iloc[i][\"text\"]\n",
    "    result = classifer.predict(prompt)\n",
    "    answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "    if \"positive\" in answer:\n",
    "        y_pred.append(\"positive\")\n",
    "    elif \"negative\" in answer:\n",
    "        y_pred.append(\"negative\")\n",
    "    elif \"neutral\" in answer:\n",
    "        y_pred.append(\"neutral\")\n",
    "    else:\n",
    "        y_pred.append(\"none\")\n",
    "X_test[\"pred\"] =  y_pred\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"x_test_with_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_map = {'negative': 0, 'neutral': 1, 'none':1, 'positive': 2}\n",
    "error_analysis_data = X_test.copy()\n",
    "predictions = X_test[\"pred\"]\n",
    "y_test = X_test[\"sentiment\"]\n",
    "error_analysis_data['prediction'] = list(map(churn_map.get, predictions))\n",
    "error_analysis_data['target'] = list(map(churn_map.get, y_test))\n",
    "\n",
    "error_analysis_data['target_prediction_match'] = error_analysis_data['target'].eq(error_analysis_data['prediction'])\n",
    "\n",
    "categorical_predictors = list(set(categorical_columns).intersection(set(error_analysis_data.columns)))\n",
    "for categorical_predictor in categorical_predictors:\n",
    "    print(error_analysis_data.groupby(categorical_predictor)['target_prediction_match'].mean().reset_index().to_string())\n",
    "    sns.catplot(data=error_analysis_data, x=categorical_predictor, y=\"target_probability_diff\", kind=\"box\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 622510,
     "sourceId": 1192499,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3090,
     "sourceId": 4295,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
